---
layout:     post
title:      初识大数据生态圈
subtitle:   概述
date:       2021-03-31
author:     QuanLi
header-img: img/post-bg-kuaidi.jpg
catalog: true
tags:
    - 大数据
---

# 初识大数据生态圈

​	学完了李智慧老师的大数据基础课程，对大数据有了一个整体的初步认识，熟悉了大数据生态圈相关的主流属性。做一个日报总结。

## 大数据处理流程

![image-20210331105948374](D:\MyConfiguration\li2.quan\AppData\Roaming\Typora\typora-user-images\image-20210331105948374.png)

### 数据采集

​	大数据是用来处理海量数据的，那么数据的来源主要是什么？大数据的数据主要是来源于**数据库导入**、**日志文件导入**、**前端埋点采集**、**爬虫系统**

##### 数据库导入

​	大数据技术出现前，人们习惯了使用关系型数据库作为分析处理的工具，当大数据技术出现就会很自然地想到，将关系型数据库的技巧方法转移到大数据技术上，于是像Hive、Spark SQL这样的大数据SQL产品就出现了，这些产品可以提供像关系型数据库一样的SQL操作，虽然Hive这样的产品可以提供SQL操作，但是互联网产生的数据还是只能记录在MySQL这样的数据库中，所以需要工具将关系型数据库中的数据批量导入到大数据平台上。

​	目前主流的数据库导入工具有**Sqoop和Canal**。**Sqoop适合离线批量导入关系数据库的数据，主要是解决了数据迁移的问题，**它能够通过简单的命令将关系型数据库中的数据导入到 HDFS 、Hive 或 HBase 中，或者从 HDFS 、Hive 导出到关系型数据库上。**Canle适合实时导入关系数据库的数据。**Canal 是阿里巴巴开源的一个 MySQL binlog 获取工具，Canal将自己伪装成MySQL从库，从MySQL获取binlog,我们只要开发一个Canal客户端程序，就可以解析出MySQL的写操作数据，将这些数据交给大数据计算处理引擎，就可以实现对MySQL数据的实时处理了。

##### 从日志文件导入

​	日志也是数据的重要来源之一，应用程序日志记录了系统运行时各种程序的执行情况，另一方面也记录了用户的业务处理轨迹。**Flume是一个大数据日志收集的常用工具。**

##### 前端埋点采集

​	前端埋点数据采集也是互联网应用大数据的重要来源之一，用户的某些前端行为并不会产生后端请求，比如用户在一个页面的停留时间、用户拖动页面的速度、用户选中一个复选框然后又取消了。这些信息对于大数据处理，对于分析用户行为，进行智能推荐都很有价值。但是这些数据必须通过前端埋点获得，所谓前端埋点，就是应用前端为了进行数据统计和分析而采集数据。

##### 爬虫系统

通过网络爬虫获取外部数据也是公司大数据的重要来源之一。

### 数据批处理和数据流处理

​	像**MapReduce**和**Spark**这种计算框架处理的业务场景都成为数据批处理，因为计算的对象是历史数据，不是实时的数据。

​	如果需要对大量数据进行即时计算，可以使用**Storm、Flink、Spark Streaming** 等流计算框架来满足此类大数据应用的场景。大数据流处理中用的比较多得另一个框架是**Kafka**,它可以用于消峰，避免在秒杀等高并发场景下并发数据对流数据造成冲击。

### 数据存储

​	收集到数据后，下一个问题就是：数据该如何进行存储？通常大家最为熟知是 MySQL、Oracle 等传统的关系型数据库，它们的优点是能够快速存储结构化的数据，并支持随机访问。但大数据的数据结构通常是半结构化（如日志数据）、甚至是非结构化的（如视频、音频数据），为了解决海量半结构化和非结构化数据的存储，衍生了 Hadoop HDFS 、KFS、GFS 等分布式文件系统，它们都能够支持结构化、半结构和非结构化数据的存储，并可以通过增加机器进行横向扩展。

​	分布式文件系统完美地解决了海量数据存储的问题，但是一个优秀的数据存储系统需要同时考虑数据存储和访问两方面的问题，比如你希望能够对数据进行随机访问，这是传统的关系型数据库所擅长的，但却不是分布式文件系统所擅长的，那么有没有一种存储方案能够同时兼具分布式文件系统和关系型数据库的优点，基于这种需求，就产生了 HBase、MongoDB。

## HDFS原理

​	HDFS的设计目标是管理数以千计的服务器，数以万计的磁盘，将这么大的服务器计算资源当作一个单一的存储系统来进行管理。HDFS采用了主从结构（Master/Slave），一个HDFS集群是由一个NameNode和若干个DataNode组成的，其中**NameNode是作为主服务器**，负责管理文件系统命名空间和客户端对文件的访问操作；DataNode 负责文件数据的存储和读写操作，HDFS 将文件数据分割成若干数据块（Block），**每个 DataNode 存储一部分数据块**，这样文件就分布存储在整个 HDFS 服务器集群中。

![image-20210331153118218](D:\MyConfiguration\li2.quan\AppData\Roaming\Typora\typora-user-images\image-20210331153118218.png)

## MapReduce 的具体处理过程

​	大数据的核心理念是“**移动计算比移动数据更加划算**”，不管是关系代数运算（SQL 计算），还是矩阵运算（图计算），大数据领域几乎所有的计算需求都可以通过 MapReduce 编程来实现。MapReduce 既是一个编程模型，又是一个计算框架。也就是说，开发人员必须基于 MapReduce 编程模型进行编程开发，然后将程序通过 MapReduce 计算框架分发到 Hadoop 集群中运行。

​	在 Map 阶段为每个数据块分配一个 Map 计算任务，然后将所有 map 输出的 Key 进行合并，相同的 Key 及其对应的 Value 发送给同一个 Reduce 任务去处理。通过这两个阶段，工程师只需要遵循 MapReduce 编程模型就可以开发出复杂的大数据计算程序。

### 实现MapReduce的关键问题

##### 如何为每一个数据块分配Map计算任务

​	也就是代码如何发送到数据块所在服务器，发送后如何启动，启动后如何知道自己需要计算的数据在文件什么位置。**JobTracker 进程和 TaskTracker 进程是主从关系**，主服务器通常只有一台（或者另有一台备机提供高可用服务，但运行时只有一台服务器对外提供服务，真正起作用的只有一台），从服务器可能有几百上千台，所有的从服务器听从主服务器的控制和调度安排。主服务器负责为应用程序分配服务器资源以及作业执行的调度，而具体的计算操作则在从服务器上完成。

1. JobTracker 进程。这类进程根据要处理的输入数据量，命令下面提到的 TaskTracker 进程启动相应数量的 Map 和 Reduce 进程任务，并管理整个作业生命周期的任务调度和监控。这是 Hadoop 集群的常驻进程，需要注意的是，JobTracker 进程在整个 Hadoop 集群全局唯一。
2. .TaskTracker 进程。这个进程负责启动和管理 Map 进程以及 Reduce 进程。因为需要每个数据块都有对应的 map 函数，TaskTracker 进程通常和 HDFS 的 DataNode 进程启动在同一个服务器。也就是说，Hadoop 集群中绝大多数服务器同时运行 DataNode 进程和 TaskTracker 进程。

##### 不同服务器的Map输出的键值对如何聚合到一起发送给Reduce处理

​	在 map 输出与 reduce 输入之间，MapReduce 计算框架处理数据合并与连接操作，这个操作有个专门的词汇叫 shuffle。每个 Map 任务的计算结果都会写入到本地文件系统，等 Map 任务快要计算完成的时候，MapReduce 计算框架会启动 shuffle 过程，在 Map 任务进程调用一个 Partitioner 接口，对 Map 产生的每个 进行 Reduce 分区选择，然后通过 HTTP 通信发送给对应的 Reduce 进程。这样不管 Map 位于哪个服务器节点，相同的 Key 一定会被发送给相同的 Reduce 进程。Reduce 任务进程对收到的 进行排序和合并，相同的 Key 放在一起，组成一个 传递给 Reduce 执行。

![image-20210331161611353](D:\MyConfiguration\li2.quan\AppData\Roaming\Typora\typora-user-images\image-20210331161611353.png)

## 分布式集群资源调度框架 Yarn

​	在 MapReduce 应用程序的启动过程中，最重要的就是要把 MapReduce 程序分发到大数据集群的服务器上，在 Hadoop 1 中，这个过程主要是通过 TaskTracker 和 JobTracker 通信来完成。这种架构方案的主要缺点是，**服务器集群资源调度管理和 MapReduce 执行过程耦合在一起**，如果想在当前集群中运行其他计算任务，比如 Spark 或者 Storm，就无法统一使用集群中的资源了。

​	在 Hadoop 早期的时候，大数据技术就只有 Hadoop 一家，这个缺点并不明显。但随着大数据技术的发展，各种新的计算框架不断出现，我们不可能为每一种计算框架部署一个服务器集群，而且就算能部署新集群，数据还是在原来集群的 HDFS 上。**所以我们需要把 MapReduce 的资源管理和计算框架分开**，这也是 **Hadoop 2 最主要的变化，就是将 Yarn 从 MapReduce 中分离出来，成为一个独立的资源调度框架。**

​	从图上看，Yarn 包括两个部分：一个是资源管理器（Resource Manager），一个是节点管理器（Node Manager）。这也是 Yarn 的两种主要进程：ResourceManager 进程负责整个集群的资源调度管理，通常部署在独立的服务器上；NodeManager 进程负责具体服务器上的资源和任务管理，在集群的每一台计算服务器上都会启动，基本上跟 HDFS 的 DataNode 进程一起出现。

![image-20210331163941004](D:\MyConfiguration\li2.quan\AppData\Roaming\Typora\typora-user-images\image-20210331163941004.png)

## Spark

​	使用MapReduce编程的时候思考的是，将计算逻辑用Map和Reduce两个阶段实现，map和reduce的输入输出是什么。而Spark强调直接针对数据进行编程，将大规模数据集合抽象成一个 RDD 对象，然后在这个 RDD 上进行各种计算处理，得到一个新的 RDD，继续计算处理，直到得到最后的结果数据。**所以 Spark 可以理解成是面向对象的大数据计算。**我们在进行 Spark 编程的时候，思考的是一个 RDD 对象需要经过什么样的操作，转换成另一个 RDD 对象，思考的重心和落脚点都在 RDD 上。RDD 是 Spark 的核心概念，是弹性数据集（Resilient Distributed Datasets）的缩写。RDD 上定义的函数分两种，一种是转换（transformation）函数，这种函数的返回值还是 RDD；另一种是执行（action）函数，这种函数不再返回 RDD。

### Spark生态体系

Spark 也有自己的生态体系，以 Spark 为基础，有支持 SQL 语句的 Spark SQL，有支持流计算的 Spark Streaming，有支持机器学习的 MLlib，还有支持图计算的 GraphX。利用这些产品，Spark 技术栈支撑起大数据分析、大数据机器学习等各种大数据应用场景。

![image-20210331171055769](D:\MyConfiguration\li2.quan\AppData\Roaming\Typora\typora-user-images\image-20210331171055769.png)

### Spark为什么比MapReduce更高效

​	首先和 MapReduce 一个应用一次只运行一个 map 和一个 reduce 不同，**Spark 可以根据应用的复杂程度，分割成更多的计算阶段（stage），这些计算阶段组成一个有向无环图 DAG，Spark 任务调度器可以根据 DAG 的依赖关系执行计算阶段。**

​	所谓 DAG 也就是有向无环图，**就是说不同阶段的依赖关系是有向的**，**计算过程只能沿着依赖关系方向执行**，被依赖的阶段执行完成之前，依赖的阶段不能开始执行，同时，这个依赖关系不能有环形依赖，否则就成为死循环了。下面这张图描述了一个典型的 Spark 运行 DAG 的不同阶段。正因为如此，Spark非常适合用来迭代计算。

![image-20210331171630491](D:\MyConfiguration\li2.quan\AppData\Roaming\Typora\typora-user-images\image-20210331171630491.png)

​	从图上看，整个应用被切分成 3 个阶段，阶段 3 需要依赖阶段 1 和阶段 2，阶段 1 和阶段 2 互不依赖。Spark 在执行调度的时候，先执行阶段 1 和阶段 2，完成以后，再执行阶段 3。如果有更多的阶段，Spark 的策略也是一样的。只要根据程序初始化好 DAG，就建立了依赖关系，然后根据依赖关系顺序执行各个计算阶段，Spark 大数据应用的计算就完成了。

​	Hadoop MapReduce 简单粗暴地根据 shuffle 将大数据计算分成 Map 和 Reduce 两个阶段，然后就算完事了。而 Spark 更细腻一点，将前一个的 Reduce 和后一个的 Map 连接起来，当作一个阶段持续计算，形成一个更加优雅、高效的计算模型，虽然其本质依然是 Map 和 Reduce。但是**这种多个计算阶段依赖执行的方案可以有效减少对 HDFS 的访问，减少作业的调度执行次数，因此执行速度也更快。**

### Spark总结

​	Spark的优点就是能够动态根据计算逻辑的复杂度进行不断的拆分子任务，而实现在一个应用中处理所有的逻辑，而不像MapReduce需要启动多个应用进行计算。

​	总结来说，Spark 有三个主要特性：**RDD 的编程模型更简单**，**DAG 切分的多阶段计算过程更快速**，**使用内存存储中间计算结果更高效**。这三个特性使得 Spark 相对 Hadoop MapReduce 可以有更快的执行速度，以及更简单的编程实现。

## NoSQL 系统 HBase

​	我们先来看看 HBase 的架构设计。HBase 为可伸缩海量数据储存而设计，实现面向在线业务的实时数据访问延迟。HBase 的伸缩性主要依赖其可分裂的 HRegion 及可伸缩的分布式文件系统 HDFS 实现。

![image-20210331175942013](D:\MyConfiguration\li2.quan\AppData\Roaming\Typora\typora-user-images\image-20210331175942013.png)

​	HRegion 是 HBase 负责数据存储的主要进程，应用程序对数据的读写操作都是通过和 HRegion 通信完成。上面是 HBase 架构图，我们可以看到在 HBase 中，数据以 HRegion 为单位进行管理，也就是说应用程序如果想要访问一个数据，必须先找到 HRegion，然后将数据读写操作提交给 HRegion，由 HRegion 完成存储层面的数据操作。

​	HRegion 是 HBase 负责数据存储的主要进程，应用程序对数据的读写操作都是通过和 HRegion 通信完成。上面是 HBase 架构图，我们可以看到在 HBase 中，数据以 HRegion 为单位进行管理，也就是说应用程序如果想要访问一个数据，必须先找到 HRegion，然后将数据读写操作提交给 HRegion，由 HRegion 完成存储层面的数据操作。

​	每个 HRegion 中存储一段 Key 值区间[key1, key2) 的数据，所有 HRegion 的信息，包括存储的 Key 值区间、所在 HRegionServer 地址、访问端口号等，都记录在 HMaster 服务器上。为了保证 HMaster 的高可用，HBase 会启动多个 HMaster，并通过 ZooKeeper 选举出一个主服务器。

## ZooKeeper如何保证数据的一致性

​	ZooKeeper 主要提供数据的一致性服务，**其实现分布式系统的状态一致性依赖一个叫 Paxos 的算法**。**Paxos 算法在多台服务器通过内部的投票表决机制决定一个数据的更新与写入**。

![image-20210331181802047](D:\MyConfiguration\li2.quan\AppData\Roaming\Typora\typora-user-images\image-20210331181802047.png)

​	应用程序连接到任意一台服务器后提起状态修改请求（也可以是获得某个状态锁的请求），从图上看也就是服务器 1，会将这个请求发送给集群中其他服务器进行表决。如果某个服务器同时收到了另一个应用程序同样的修改请求，它可能会拒绝服务器 1 的表决，并且自己也发起一个同样的表决请求，那么其他服务器就会根据时间戳和服务器排序规则进行表决。**表决结果会发送给其他所有服务器**，最终发起表决的服务器也就是服务器 1，**会根据收到的表决结果决定该修改请求是否可以执行**，事实上，**只有在收到多数表决同意的情况下才会决定执行**。当有多个请求同时修改某个数据的情况下，服务器的表决机制保证只有一个请求会通过执行，从而保证了数据的一致性。ZooKeeper 作为一个数据一致性解决方案产品，**事实上是牺牲了部分可用性，换来的数据一致性。**在 Paxos 算法中，如果某个应用程序连接到一台服务器，但是这台服务器和其他服务器的网络连接出现问题，那么这台服务器将返回一个错误，要求应用程序重新请求。

