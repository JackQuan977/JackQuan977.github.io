---
layout:     post
title:      Volatile关键字
subtitle:   概述
date:       2021-05-28
author:     QuanLi
header-img: img/post-bg-kuaidi.jpg
catalog: true
tags:
    - 多线程
---

# Volatile关键字

​	volatile这个关键字一般通常用于并发编程中，**是 Java 虚拟机提供的轻量化同步机制**。**volatile 能够保证共享变量之间的 `可见性`**，共享变量是存在堆区的，而堆区又与内存模型有关，所以我们要聊 volatile ，就需要首先了解一下 Java 内存模型。Java 中的内存模型是 JVM 提供的，而 JVM 又是和内存进行交互的，所以在聊 Java 内存模型前，我们还需要了解一下操作系统层面中内存模型的相关概念。

### 先从内存模型谈起

​	计算机在执行程序时，会从内存中读取数据，然后加载到 CPU 中运行。由于 CPU 执行指令的速度要比从内存中读取和写入的速度快的多，所以如果每条指令都要和内存交互的话，会大大降低 CPU 的运行速度，造成昂贵的 CPU 性能损耗，为了解决这种问题，设计了 **CPU 高速缓存**。有了 CPU 高速缓存后，CPU 就不再需要频繁的和内存交互了，有高速缓存就行了，而 CPU 高速缓存，就是我们经常说的 L1 、L2、L3 cache。

​	当程序在运行过程中，会将运算需要的数据从主存复制一份到 CPU 的高速缓存中，在 CPU 进行计算时就可以直接从它的高速缓存读写数据，当运算结束之后，再将高速缓存中的数据刷新到主存中。

​	就拿我们常说的来举例子

~~~JAVA
i = i + 1 
~~~

​	当 CPU 执行这条语句时，会先从内存中读取 i 的值，复制一份到高速缓存当中，然后 CPU 执行指令对 i 进行加 1 操作，再将数据写入高速缓存，最后将高速缓存中 i 最新的值刷新到主存当中。

​	这个代码在单线程中运行是没有任何问题的，但是在多线程中运行就会有问题了，**因为每个 CPU 都可以运行一条线程**，线程就是程序的顺序执行流，因此**每个线程运行时有自己的高速缓存**。

​	比如同时有 2 个线程执行这段代码，假如初始时 i 的值为 0，那么我们希望两个线程执行完之后 i 的值变为 2，但是事实会是这样吗？

​	可能存在下面一种情况：初始时，两个线程分别读取 i 的值存入各自所在的 CPU 高速缓存中，然后线程 1 执行加 1 操作，把 i 的最新值 1 写入到内存。此时线程 2 的高速缓存当中 i 的值还是 0，进行加 1 操作之后，i 的值为 1，然后线程 2 把 i 的值写入内存。

​	最终结果 i 的值是 1，而不是 2。这就是著名的**缓存一致性**问题。通常称这种被多个线程访问的变量为共享变量。

​	也就是说，如果一个变量在多个 CPU 中都存在缓存（一般在多线程编程时才会出现），就很可能存在缓存不一致的问题。

### Java 内存模型 JMM

​	我们上面说到，共享变量会存在缓存不一致的问题，缓存不一致问题换种说法就是**线程安全问题**，那么共享变量在 Java 中是如何存在的呢？JVM 中有没有提供线程安全的变量或者数据呢？

​	这就要聊聊 Java 内存模型的问题了，图示如下

![image-20210714171940557](https://i.loli.net/2021/07/14/JynesLTXr2iS6Dx.png)

- `虚拟机栈` : Java 虚拟机栈是线程私有的数据区，Java 虚拟机栈的生命周期与线程相同，虚拟机栈也是局部变量的存储位置。方法在执行过程中，会在虚拟机栈种创建一个 `栈帧(stack frame)`。
- `本地方法栈`: 本地方法栈也是线程私有的数据区，本地方法栈存储的区域主要是 Java 中使用 `native` 关键字修饰的方法所存储的区域。
- `程序计数器`：程序计数器也是线程私有的数据区，这部分区域用于存储线程的指令地址，用于判断线程的分支、循环、跳转、异常、线程切换和恢复等功能，这些都通过程序计数器来完成。
- `方法区`：方法区是各个线程共享的内存区域，它用于存储虚拟机加载的 类信息、常量、静态变量、即时编译器编译后的代码等数据。
- `堆`： 堆是线程共享的数据区，堆是 JVM 中最大的一块存储区域，所有的对象实例都会分配在堆上
- `运行时常量池`：运行时常量池又被称为 `Runtime Constant Pool`，这块区域是方法区的一部分，它的名字非常有意思，它并不要求常量一定只有在编译期才能产生，也就是并非编译期间将常量放在常量池中，运行期间也可以将新的常量放入常量池中，String 的 intern 方法就是一个典型的例子。

根据上面的描述可以看到，会产生缓存不一致问题（线程安全问题）的有**堆区和方法区**。而虚拟机栈、本地方法栈、程序计数器是线程私有，由线程封闭的原因，它们不存在线程安全问题。

> 针对线程安全问题，有没有解决办法呢？

​	一般情况下，Java 中解决缓存不一致的方法有两种，第一种就是 `synchronized` 使用的**总线锁**方式，也就是在总线上声言 `LOCK#` 信号；第二种就是著名的 `MESI` 协议。这两种都是硬件层面提供的解决方式。

​	我们先来说一下第一种**总线锁**的方式。**通过在总线上声言 LOCK# 信号，能够有效的阻塞其他 CPU 对于总线的访问，从而使得只能有一个 CPU 访问变量所在的内存。**在上面的 i = i + 1 代码示例中，在代码执行的过程中，声言了 LOCK# 信号后，那么只有等待 i = i + 1 的结果执行完毕并应用到内存后，总线锁才会解开，其他 CPU 才能够继续访问内存中的变量，再继续执行后面的代码，这样就解决了缓存不一致问题。

![image-20210714172516597](https://i.loli.net/2021/07/14/RLKcOgr4adYuhH6.png)



​	MESI 协议就是缓存一致性协议，即 **Modified（被修改）Exclusive（独占的） Shared（共享的） Or Invalid（无效的）**。MESI 的基本思想就是如果发现 CPU 操作的是共享变量，其他 CPU 中也会出现这个共享变量的副本，在 CPU 执行代码期间，会发出信号通知其他 CPU 自己正在修改共享变量，其他 CPU 收到通知后就会把自己的共享变量置为无效状态。

![image-20210714172730625](https://i.loli.net/2021/07/14/nBEQRApOucU6Wsi.png)



### 并发编程中的三个主要问题

#### 可见性问题

​	在单核 CPU 时代，所有的线程共用一个 CPU，CPU 缓存和内存的一致性问题容易解决，我们还拿上面的 i = 1 + 1 来举例，CPU 和 内存之间如果用图来表示的话我想会是下面这样。

![image-20210714173442083](https://i.loli.net/2021/07/14/sRwCgZUV8TJnhax.png)



​	在多核时代，每个核都能够独立的运行一个线程，每个 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程使用的是不同的 CPU 缓存。

![image-20210714173553342](https://i.loli.net/2021/07/14/61WKlNUgYyID7ex.png)



​	因为 i 没有经过任何线程安全措施的保护，多个线程会并发修改 i 的值，所以我们认为 i 不是线程安全的，导致这种结果的出现是由于 aThread 和 bThread 中读取的 i 值彼此不可见，所以这是由于 `可见性` 导致的线程安全问题。

#### 原子性问题

​	当两个线程开始运行后，每个线程都会把 i 的值读入到 CPU 缓存中，再执行 + 1 操作，然后把 + 1 之后的值写入内存。因为线程间都有各自的虚拟机栈和程序计数器，他们彼此之间没有数据交换，所以当 aThread 执行 + 1 操作后，会把数据写入到内存，同时 bThread 执行 + 1 操作后，也会把数据写入到内存，因为 CPU 时间片的执行周期是不确定的，所以会出现当 aThread 还没有把数据写入内存时，bThread 就会读取内存中的数据，然后执行 + 1操作，再写回内存，从而覆盖 i 的值。

![image-20210714173835044](https://i.loli.net/2021/07/14/ZvISoU3lJpfCj5Q.png)

#### 有序性问题

​	在并发编程中还有带来让人非常头疼的 `有序性` 问题，有序性顾名思义就是顺序性，**在计算机中指的就是指令的先后执行顺序**。一个非常显而易见的例子就是 JVM 中的`类加载`。

在执行程序的过程中，为了提高性能，编译器和处理器通常会对指令进行重排序。重排序主要分为三类

- **编译器优化的重排序**：编译器在不改变单线程语义的情况下，会对执行语句进行重新排序。
- **指令集重排序**：现代操作系统中的处理器都是并行的，如果执行语句之间不存在数据依赖性，处理器可以改变语句的执行顺序
- **内存重排序**：由于处理器会使用读/写缓冲区，出于性能的原因，内存会对读/写进行重排序

也就是说，要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。

### volatile 的实现原理

​	在并发编程中，最需要处理的就是线程之间的`通信`和线程间的`同步`问题，上面的可见性、原子性、有序性也是这两个问题带来的。

#### 可见性

​	而 volatile 就是为了解决这些问题而存在的。Java 语言规范对 volatile 下了定义：Java 语言为了确保能够安全的访问共享变量，提供了 volatile 这个关键字，volatile 是一种**轻量级同步**机制，它并不会对共享变量进行加锁，但在某些情况下要比加锁**更加方便**，如果一个字段被声明为 volatile，Java 线程内存模型能够确保所有线程访问这个变量的值都是一致的。

​	一旦共享变量被 volatile 修饰后，就具有了下面两种含义

1. **保证了这个字段的可见性**，也就是说所有线程都能够"看到"这个变量的值，如果某个 CPU 修改了这个变量的值之后，其他 CPU 也能够获得通知。
2. **能够禁止指令的重排序**

下面我们来看一段代码，这也是我们编写并发代码中经常会使用到的

~~~java
boolean isStop = false;
while(!isStop){
    ...
}
 
isStop = true;
~~~

​	在这段代码中，如果线程一正在执行 while 循环，而线程二把 isStop 改为 true 之后，转而去做其他事情，因为线程一并不知道线程二把 isStop 改为 true ，所以线程一就会一直运行下去。

​	如果 isStop 用 volatile 修饰之后，那么事情就会变的不一样了。

使用 volatile 修饰了 isStop 之后，在线程二把 isStop 改为 true 之后，**会强制将其写入内存**，并且会把线程一中 isStop 的值置为无效（这个值实际上是在缓存在 CPU 中的缓存行里），**当线程一继续执行代码的时候，会从内存中重新读取 isStop 的值，此时 isStop 的值就是正确的内存地址的值。**

​	volatile 有下面两条实现原则，其实这两条原则我们在上面介绍的时候已经提过了，一种是总线锁的方式，我们后面说总线锁的方式开销比较大，所以后面设计人员做了优化，采用了锁缓存的方式。另外一种是 MESI 协议的方式。

- 在 IA-32 架构软件开发者的手册中，有一种 Lock 前缀指令，这种指令能够声言 LOCK# 信号，在最近的处理器中，LOCK# 信号用于锁缓存，等到指令执行完毕后，会把缓存的内容写回内存，这种操作一般又被称为**缓存锁定**。
- 当缓存写回内存后，IA-32 和 IA-64 处理器会使用 **MESI 协议**控制内部缓存和其他处理器一致。IA-32 和 IA-64 处理器能够嗅探其他处理器访问系统内部缓存，当内存值修改后，处理器会从内存中重新读取内存值进行新的缓存行填充。

由此可见，volatile 能够保证线程的可见性。

> 那么 volatile 能不能保证有序性呢？

这里就需要和你聊一聊 volatile 对有序性的影响了

#### 有序性

​	上面提到过，重排序分为编译器重排序、处理器重排序和内存重排序。我们说的 volatile 会禁用指令重排序，实际上 volatile 禁用的是编译器重排序和处理器重排序。

​	下面是 volatile 禁用重排序的规则

![image-20210714175947899](https://i.loli.net/2021/07/14/sobFO538EUMaCpj.png)

从这个表中可以看出来，读写操作有四种，即不加任何修饰的普通读写和使用 volatile 修饰的读写。

从这个表中，我们可以得出下面这些结论

- 只要第二个操作（这个操作就指的是代码执行指令）是 volatile 修饰的写操作，那么无论第一个操作是什么，都不能被重排序。
- 当第一个操作是 volatile 读时，不管第二个操作是什么，都不能进行重排序。
- 当第一个操作是 volatile 写之后，第二个操作是 volatile 读/写都不能重排序。

为了实现这种有序性，编译器会在生成字节码中，会在指令序列中插入**内存屏障**来禁止特定类型的处理器重排序。

> 这里我们先来了解一下内存屏障的概念。

​	内存屏障也叫做`栅栏`，它是一种**底层原语**。它使得 CPU 或编译器在对内存进行操作的时候, 要严格按照一定的顺序来执行, 也就是说**在 memory barrier 之前的指令和 memory barrier 之后的指令不会由于系统优化等原因而导致乱序。**

​	内存屏障提供了两个功能。首先，它们通过确保从另一个 CPU 来看屏障的两边的所有指令都是正确的程序顺序；其次它们可以实现内存数据可见性，确保内存数据会同步到 CPU 缓存子系统。

​	不同计算机体系结构下面的内存屏障也不一样，通常需要认真研读硬件手册来确定，所以我们的主要研究对象是基于 x86 的内存屏障，通常情况下，硬件为我们提供了四种类型的内存屏障。

- LoadLoad 屏障

它的执行顺序是 Load1 ； LoadLoad ；Load2 ，其中的 Load1 和 Load2 都是加载指令。LoadLoad 指令能够确保执行顺序是在 Load1 之后，Load2 之前，LoadLoad 指令是一个比较有效的防止看到旧数据的指令。

- StoreStore 屏障

它的执行顺序是 Store1 ；StoreStore ；Store2 ，和上面的 LoadLoad 屏障的执行顺序相似，它也能够确保执行顺序是在 Store1 之后，Store2 之前。

- LoadStore 屏障

它的执行顺序是 Load1 ； StoreLoad ； Store2 ，保证 Load1 的数据被加载在与这数据相关的 Store2 和之后的 store 指令之前。

- StoreLoad 屏障

它的执行顺序是 Store1 ； StoreLoad ； Load2 ，保证 Store1 的数据被其他 CPU 看到，在数据被 Load2 和之后的 load 指令加载之前。也就是说，它有效的防止所有 barrier 之前的 stores 与所有 barrier 之后的 load 乱序。

JMM 采取了保守策略来实现内存屏障，JMM 使用的内存屏障如下

![image-20210714182147643](https://i.loli.net/2021/07/14/ePZqQgKNkroSF3C.png)

### 关键概念

在 volatile 实现可见性和有序性的过程中，有一些关键概念:

- 缓冲行：英文概念是 **cache line**，它是缓存中可以分配的最小存储单位。因为数据在内存中不是以独立的项进行存储的，而是以临近 64 字节的方式进行存储。
- 缓存行填充：**cache line fill**，当 CPU 把内存的数据载入缓存时，会把临近的共 64 字节的数据一同放入同一个 Cache line，因为局部性原理：临近的数据在将来被访问的可能性大。
- 缓存命中：**cache hit**，当 CPU 从内存地址中提取数据进行缓存行填充时，发现提取的位置仍然是上次访问的位置，此时 CPU 会选择从缓存中读取操作数，而不是从内存中取。
- 写命中：**write hit** ，当处理器打算将操作数写回到内存时，首先会检查这个缓存的内存地址是否在缓存行中，如果存在一个有效的缓存行，则处理器会将这个操作数写回到缓存，而不是写回到内存，这种方式被称为写命中。
- 内存屏障：**memory barriers**，是一组硬件指令，是 volatile 实现有序性的基础。
- 原子操作：**atomic operations**，是一组不可中断的一个或者一组操作。

### volatile应用

​	从程序代码简易性和可伸缩性角度来看，你可能更倾向于使用 volatile 而不是锁，因为 volatile 写起来更方便，并且 **volatile 不会像锁那样造成线程阻塞**，而且如果程序中的**读操作的使用远远大于写操作的话，volatile 相对于锁还更加具有性能优势**。

#### 状态标识

AQS

#### 双重检查锁

